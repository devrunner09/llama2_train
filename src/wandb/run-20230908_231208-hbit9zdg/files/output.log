
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.48s/it]
/workdir/env/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
--> Model /workdir/ViLLM/base_models/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9
--> /workdir/ViLLM/base_models/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9 has 262.41024 Million params
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
--> Training Set Length = 67017
/workdir/env/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 0:   0%|[34m                                                               [39m| 0/2094 [00:00<?, ?it/s]
Training Epoch: 0:   0%|[34m                                                               [39m| 0/2094 [00:00<?, ?it/s]/workdir/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")




































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Training Epoch: 0/3, step 2093/2094 completed (loss: 0.6079181432723999): : 2191371it [9:50:56, 123.37it/s]
evaluating Epoch:   0%|[32mâ–Ž                                                                    [39m| 1/200 [00:00<02:44,  1.21it/s]
Saving model at epoch: 0
Max CUDA memory allocated was 14 GB
Max CUDA memory reserved was 17 GB
Peak active CUDA memory was 14 GB
Cuda Malloc retires : 0



































































Training Epoch: 0/3, step 2093/2094 completed (loss: 0.6079181432723999): : 2191371it [9:53:12, 61.57it/s] 00:00,  1.53it/s]
Training Epoch: 1:   0%|[34m                                                                           [39m| 0/2094 [00:00<?, ?it/s]
 eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')
Epoch 1: train_perplexity=1.8020, train_epoch_loss=0.5889, epcoh time 35457.088187632035s







































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Training Epoch: 1/3, step 2092/2094 completed (loss: 0.4737226068973541): : 2191371it [9:50:48, 123.49it/s]
Saving model at epoch: 1
Max CUDA memory allocated was 14 GB
Max CUDA memory reserved was 17 GB
Peak active CUDA memory was 14 GB
Cuda Malloc retires : 0



Training Epoch: 1/3, step 2093/2094 completed (loss: 0.568945050239563): : 2191371it [9:53:10, 61.57it/s] 00<?, ?it/s]
 eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')
Epoch 2: train_perplexity=1.7234, train_epoch_loss=0.5443, epcoh time 35449.32702645502s





















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Training Epoch: 2/3, step 2092/2094 completed (loss: 0.458271861076355): : 2189278it [9:50:40, 123.41it/s]
Saving model at epoch: 2
Max CUDA memory allocated was 14 GB
Max CUDA memory reserved was 17 GB
Peak active CUDA memory was 14 GB
Cuda Malloc retires : 0
Training Epoch: 2/3, step 2093/2094 completed (loss: 0.5444961786270142): : 2191371it [9:50:43, 123.45it/s]

































































evaluating Epoch:  99%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–[39m| 198/200 [02:10<00:01,  1.47it/s]
 eval_ppl=tensor(nan, device='cuda:0') eval_epoch_loss=tensor(nan, device='cuda:0')
Epoch 3: train_perplexity=1.6764, train_epoch_loss=0.5167, epcoh time 35443.633355999016s
Key: avg_train_prep, Value: 1.733919620513916
Key: avg_train_loss, Value: 0.5499405264854431
Key: avg_eval_prep, Value: nan
Key: avg_eval_loss, Value: inf
Key: avg_epoch_time, Value: 35450.01619002869

Training Epoch: 2/3, step 2093/2094 completed (loss: 0.5444961786270142): : 2191371it [9:52:55, 61.60it/s] [32mâ–ˆâ–ˆâ–ˆ[39m| 200/200 [02:12<00:00,  1.43it/s]