

Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████| 2/2 [00:45<00:00, 22.91s/it]
/workdir/env/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/workdir/env/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
--> Model meta-llama/Llama-2-7b-hf
--> meta-llama/Llama-2-7b-hf has 262.41024 Million params
trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035
=============================
Loading dataset at: /workdir/crawl_data/merge_QA_data.json
Current max words: 1024
=============================
This is raw prompt at index 0:
Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Soạn bài thơ 10 câu nói về vẻ đẹp của mùa thu
### Response:
1. Mùa thu tươi đẹp, rực rỡ trong ánh nắng vàng.
2. Cánh đồng lúa chín vàng, cỏ cây úa tàn theo gió.
3. Cơn gió lạnh thoảng qua mang theo mù aroma hoa lá.
4. Điểm đậm của mùa thu là sắc đỏ rực rỡ của những chiếc lá khô.
5. Các loài chim trời bay đến rợp cành cây thưởng thức vẻ đẹp này.
6. Sân trường, công viên rực rỡ màu sắc của các loài hoa thơm lừng.
7. Hương trầm, hương nhài cùng với hơi lạnh của mùa thu tạo nên một khí trời thơm mát, dễ chịu.
8. Bầu trời xanh thẳm, những đám mây trôi lững thể hiện sự dịu dàng của mùa thu.
9. Rung rinh gió lùa xoa nhẹ, đưa cánh hoa mỏng bay trôi giữa không trung rực rỡ.
10. Mùa thu là mùa của sự thanh bình, tĩnh lặng, đẹp như một tác phẩm nghệ thuật hoàn hảo của thiên nhiên.
Prompt tensor size: torch.Size([73])
Sample tensor size: torch.Size([574])
Sample (after padding) tensor size: torch.Size([1024])
=============Training Example============
{'input_ids': tensor([    1, 13866,   338,  ...,     0,     0,     0]), 'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]), 'attention_mask': tensor([1., 1., 1.,  ..., 0., 0., 0.])}
=========================================
--> Training Set Length = 93146
=============================
Loading dataset at: /workdir/crawl_data/merge_QA_data.json
Current max words: 1024
=============================
/workdir/env/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 0:   0%|[34m                                                                  [39m| 0/23286 [00:00<?, ?it/s]/workdir/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
--> Validation Set Length = 200


















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Training Epoch: 0/3, step 3859/23286 completed (loss: 0.6347221732139587): : 7447870it [12:00:32, 351.26it/s]Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 277, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 243, in main
    results = train(
  File "/workdir/llama-recipes/src/llama_recipes/utils/train_utils.py", line 93, in train
    loss.backward()
  File "/workdir/env/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/workdir/env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 277, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 243, in main
    results = train(
  File "/workdir/llama-recipes/src/llama_recipes/utils/train_utils.py", line 93, in train
    loss.backward()
  File "/workdir/env/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/workdir/env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt