Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████████| 609/609 [00:00<00:00, 2.50MB/s]
Downloading (…)fetensors.index.json: 100%|████████████████████████████████████████████████████████| 26.8k/26.8k [00:00<00:00, 13.8MB/s]
Downloading shards:   0%|                                                                                        | 0/2 [00:00<?, ?it/s]













































































































































































Downloading shards:  50%|███████████████████████████████████████▌                                       | 1/2 [05:49<05:49, 349.76s/it]






























































Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [07:53<00:00, 236.61s/it]
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 271, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 95, in main
    model = LlamaForCausalLM.from_pretrained(
  File "/workdir/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3114, in from_pretrained
    raise ValueError(
ValueError:
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                        for more details.
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 271, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 95, in main
    model = LlamaForCausalLM.from_pretrained(
  File "/workdir/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3114, in from_pretrained
    raise ValueError(
ValueError:
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                        for more details.