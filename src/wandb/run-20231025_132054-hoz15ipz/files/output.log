

Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.21s/it]
/workdir/env/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/workdir/env/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
--> Model meta-llama/Llama-2-7b-hf
--> meta-llama/Llama-2-7b-hf has 262.41024 Million params
trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035
=============================
Loading dataset at: /workdir/llama-recipes/src/llama_recipes/datasets/vi-sample.json
Current max words: 1024
=============================
This is raw prompt at index 0:
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
Tạo một ví dụ về một cái gì đó đại diện cho phép ẩn dụ đã cho.
### Input:
Một con chim trong tay đáng giá gấp đôi trong bụi rậm
### Response:
Câu này dùng ẩn dụ để nói về việc giữ bí mật. Chim trong tay có nghĩa là thông tin quan trọng, và việc giữ kín nó sẽ đem lại lợi ích gấp đôi. Bụi rậm đại diện cho sự bí mật, khó tìm thấy. Ví dụ: "Anh ta giữ kín thông tin về bản hợp đồng mới nhất của công ty như con chim trong tay, vì biết rằng nó đáng giá gấp đôi trong bụi rậm ẩn giấu."
Prompt tensor size: torch.Size([127])
Sample tensor size: torch.Size([344])
Sample (after padding) tensor size: torch.Size([1024])
=============Training Example============
{'input_ids': tensor([    1, 13866,   338,  ...,     0,     0,     0]), 'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]), 'attention_mask': tensor([1., 1., 1.,  ..., 0., 0., 0.])}
=========================================
--> Training Set Length = 300
=============================
Loading dataset at: /workdir/llama-recipes/src/llama_recipes/datasets/vi-sample.json
Current max words: 1024
=============================
--> Validation Set Length = 200
/workdir/env/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 0:   0%|[34m                                                                     [39m| 0/75 [00:00<?, ?it/s]/workdir/env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")


















































































Training Epoch: 0/3, step 74/75 completed (loss: 0.47278544306755066): : 2775it [13:37,  6.51it/s]
evaluating Epoch:   0%|[32m                                                                     [39m| 0/200 [00:00<?, ?it/s]
Saving model at epoch: 0
Checkpoint name: finetuned_model_e0
Max CUDA memory allocated was 12 GB
Max CUDA memory reserved was 14 GB
Peak active CUDA memory was 12 GB
Cuda Malloc retires : 0











































































Training Epoch: 0/3, step 74/75 completed (loss: 0.47278544306755066): : 2775it [16:10,  2.86it/s]<00:00,  1.35it/s]
Training Epoch: 1:   0%|[34m                                                                     [39m| 0/75 [00:00<?, ?it/s]
 eval_ppl=tensor(1.8132, device='cuda:0') eval_epoch_loss=tensor(0.5951, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in /workdir/llama-recipes/src/llama_recipes/ directory
best eval loss on epoch 0 is 0.5950939655303955
Epoch 1: train_perplexity=1.9159, train_epoch_loss=0.6502, epcoh time 818.2369345007464s














































  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main 903it [08:57,  1.99it/s]
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 277, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 243, in main
    results = train(
  File "/workdir/llama-recipes/src/llama_recipes/utils/train_utils.py", line 93, in train
    loss.backward()
  File "/workdir/env/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/workdir/env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 277, in <module>
    fire.Fire(main)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/workdir/env/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/workdir/llama-recipes/src/llama_recipes/finetuning.py", line 243, in main
    results = train(
  File "/workdir/llama-recipes/src/llama_recipes/utils/train_utils.py", line 93, in train
    loss.backward()
  File "/workdir/env/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/workdir/env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt